{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4",
   "metadata": {},
   "source": [
    "# Save Model to MinIO\n",
    "\n",
    "Upload trained model to MinIO storage and create deployment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f6a7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from sklearn.pipeline import Pipeline\n",
    "import boto3\n",
    "from botocore.client import Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d0e1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MinIO configuration\n",
    "# Default to MinIO service in OpenShift\n",
    "NAMESPACE = os.getenv('NAMESPACE', 'spending-transaction-monitor')\n",
    "MINIO_ENDPOINT = os.getenv('MINIO_ENDPOINT', f'http://minio-service.{NAMESPACE}.svc.cluster.local:9000')\n",
    "MINIO_ACCESS_KEY = os.getenv('MINIO_ACCESS_KEY', 'minio')\n",
    "MINIO_SECRET_KEY = os.getenv('MINIO_SECRET_KEY', 'minio123')\n",
    "BUCKET_NAME = os.getenv('BUCKET_NAME', 'models')\n",
    "\n",
    "print(f\"MinIO Configuration:\")\n",
    "print(f\"  Endpoint: {MINIO_ENDPOINT}\")\n",
    "print(f\"  Bucket: {BUCKET_NAME}\")\n",
    "print(f\"  Namespace: {NAMESPACE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b4c5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize MinIO client (S3-compatible)\n",
    "s3_client = boto3.client(\n",
    "    's3',\n",
    "    endpoint_url=MINIO_ENDPOINT,\n",
    "    aws_access_key_id=MINIO_ACCESS_KEY,\n",
    "    aws_secret_access_key=MINIO_SECRET_KEY,\n",
    "    config=Config(signature_version='s3v4'),\n",
    "    region_name='us-east-1'\n",
    ")\n",
    "\n",
    "print(\"✅ MinIO client initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bucket-create",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bucket if it doesn't exist\n",
    "try:\n",
    "    s3_client.create_bucket(Bucket=BUCKET_NAME)\n",
    "    print(f\"✅ Created bucket: {BUCKET_NAME}\")\n",
    "except Exception as e:\n",
    "    if 'BucketAlreadyOwnedByYou' in str(e) or 'BucketAlreadyExists' in str(e):\n",
    "        print(f\"✅ Bucket already exists: {BUCKET_NAME}\")\n",
    "    else:\n",
    "        print(f\"Error creating bucket: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f8a9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate model version with timestamp\n",
    "model_version = datetime.now().strftime(\"%y-%m-%d-%H%M%S\")\n",
    "model_name = 'alert-recommender'\n",
    "\n",
    "print(f\"Model: {model_name}\")\n",
    "print(f\"Version: {model_version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d2e3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify model file exists\n",
    "model_path = 'models/model.pkl'\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    raise FileNotFoundError(f\"Model file not found: {model_path}. Make sure training completed successfully.\")\n",
    "\n",
    "model_size = os.path.getsize(model_path)\n",
    "print(f\"✅ Model file found: {model_size / 1024:.2f} KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-pipeline",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model artifacts\n",
    "with open(model_path, 'rb') as f:\n",
    "    model_artifacts = pickle.load(f)\n",
    "\n",
    "print(f\"Model artifacts keys: {list(model_artifacts.keys())}\")\n",
    "\n",
    "# Save KNNRecommender class as a Python module that MLServer can import\n",
    "from sklearn.base import BaseEstimator\n",
    "import numpy as np\n",
    "\n",
    "knn_module_code = '''\"\"\"\n",
    "KNN Recommender Module - for MLServer import\n",
    "\"\"\"\n",
    "from sklearn.base import BaseEstimator\n",
    "import numpy as np\n",
    "\n",
    "class KNNRecommender(BaseEstimator):\n",
    "    \"\"\"Custom KNN wrapper that provides predict() for MLServer compatibility\"\"\"\n",
    "    \n",
    "    def __init__(self, knn_model, alert_labels, alert_types, threshold=0.4):\n",
    "        self.knn_model = knn_model\n",
    "        self.alert_labels = alert_labels\n",
    "        self.alert_types = alert_types\n",
    "        self.threshold = threshold\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict recommendations for given features.\n",
    "        Returns numpy array for MLServer compatibility.\n",
    "        \"\"\"\n",
    "        k_neighbors = min(5, len(self.alert_labels))\n",
    "        distances, indices = self.knn_model.kneighbors(X, n_neighbors=k_neighbors)\n",
    "        \n",
    "        # Generate recommendations for each input\n",
    "        all_recommendations = []\n",
    "        for idx_list in indices:\n",
    "            similar_labels = self.alert_labels[idx_list]\n",
    "            probabilities = similar_labels.mean(axis=0)\n",
    "            \n",
    "            recommendations = []\n",
    "            for i, alert_type in enumerate(self.alert_types):\n",
    "                if probabilities[i] >= self.threshold:\n",
    "                    recommendations.append({\n",
    "                        'alert_type': alert_type,\n",
    "                        'probability': float(probabilities[i]),\n",
    "                        'confidence': 'high' if probabilities[i] >= 0.7 else 'medium'\n",
    "                    })\n",
    "            \n",
    "            all_recommendations.append(recommendations)\n",
    "        \n",
    "        # Return as numpy array for MLServer compatibility\n",
    "        return np.array(all_recommendations, dtype=object)\n",
    "    \n",
    "    def kneighbors(self, X, n_neighbors=None):\n",
    "        \"\"\"Forward kneighbors to the underlying KNN model\"\"\"\n",
    "        return self.knn_model.kneighbors(X, n_neighbors=n_neighbors)\n",
    "'''\n",
    "\n",
    "# Save the module file\n",
    "os.makedirs('models', exist_ok=True)\n",
    "with open('models/knn_recommender.py', 'w') as f:\n",
    "    f.write(knn_module_code)\n",
    "\n",
    "print(\"✅ Saved KNNRecommender module to models/knn_recommender.py\")\n",
    "\n",
    "# Import the class from the saved module\n",
    "import sys\n",
    "sys.path.insert(0, 'models')\n",
    "from knn_recommender import KNNRecommender\n",
    "\n",
    "# Create KNN recommender wrapper\n",
    "knn_recommender = KNNRecommender(\n",
    "    knn_model=model_artifacts['knn_model'],\n",
    "    alert_labels=model_artifacts['alert_labels'],\n",
    "    alert_types=model_artifacts['alert_types'],\n",
    "    threshold=0.4\n",
    ")\n",
    "\n",
    "# Create sklearn Pipeline for MLServer\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', model_artifacts['scaler']),\n",
    "    ('recommender', knn_recommender)\n",
    "])\n",
    "\n",
    "# Save pipeline (KNNRecommender will be pickled with module reference)\n",
    "pipeline_path = 'models/pipeline.pkl'\n",
    "with open(pipeline_path, 'wb') as f:\n",
    "    pickle.dump(pipeline, f)\n",
    "\n",
    "pipeline_size = os.path.getsize(pipeline_path)\n",
    "print(f\"✅ Created sklearn Pipeline with KNNRecommender wrapper: {pipeline_size / 1024:.2f} KB\")\n",
    "print(f\"   Pipeline steps: {[name for name, _ in pipeline.steps]}\")\n",
    "print(f\"✅ KNNRecommender will be imported from knn_recommender module by MLServer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b6c7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload pipeline and module to MinIO\n",
    "s3_model_path = f'{model_name}/'\n",
    "pipeline_key = f'{s3_model_path}pipeline.pkl'\n",
    "module_key = f'{s3_model_path}knn_recommender.py'\n",
    "\n",
    "print(f\"Uploading pipeline to s3://{BUCKET_NAME}/{pipeline_key}\")\n",
    "s3_client.upload_file(\n",
    "    pipeline_path,\n",
    "    BUCKET_NAME,\n",
    "    pipeline_key\n",
    ")\n",
    "print(\"✅ Pipeline uploaded successfully\")\n",
    "\n",
    "print(f\"Uploading KNNRecommender module to s3://{BUCKET_NAME}/{module_key}\")\n",
    "s3_client.upload_file(\n",
    "    'models/knn_recommender.py',\n",
    "    BUCKET_NAME,\n",
    "    module_key\n",
    ")\n",
    "print(\"✅ KNNRecommender module uploaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upload-settings",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and upload model-settings.json for MLServer\n",
    "model_settings = {\n",
    "    \"name\": model_name,\n",
    "    \"implementation\": \"mlserver_sklearn.SKLearnModel\",\n",
    "    \"parameters\": {\n",
    "        \"uri\": \"/mnt/models/pipeline.pkl\"\n",
    "    }\n",
    "}\n",
    "\n",
    "settings_key = f'{s3_model_path}model-settings.json'\n",
    "\n",
    "print(f\"Uploading model-settings.json to s3://{BUCKET_NAME}/{settings_key}\")\n",
    "\n",
    "s3_client.put_object(\n",
    "    Bucket=BUCKET_NAME,\n",
    "    Key=settings_key,\n",
    "    Body=json.dumps(model_settings, indent=2)\n",
    ")\n",
    "\n",
    "print(\"✅ model-settings.json uploaded\")\n",
    "print(f\"\\nModel settings:\")\n",
    "print(json.dumps(model_settings, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f0a1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify uploads\n",
    "def list_objects(prefix):\n",
    "    \"\"\"List objects in MinIO bucket with given prefix\"\"\"\n",
    "    response = s3_client.list_objects_v2(\n",
    "        Bucket=BUCKET_NAME,\n",
    "        Prefix=prefix\n",
    "    )\n",
    "    if 'Contents' in response:\n",
    "        for obj in response['Contents']:\n",
    "            print(f\"  {obj['Key']} ({obj['Size']} bytes)\")\n",
    "    else:\n",
    "        print(\"  No objects found\")\n",
    "\n",
    "print(f\"\\nObjects in s3://{BUCKET_NAME}/{s3_model_path}:\")\n",
    "list_objects(s3_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d4e5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vars.txt for next notebook\n",
    "# This file passes variables to the deployment notebook\n",
    "with open(\"vars.txt\", \"w\") as f:\n",
    "    f.write(f'model_version={model_version}\\n')\n",
    "    f.write(f'model_name={model_name}\\n')\n",
    "    f.write(f's3_bucket={BUCKET_NAME}\\n')\n",
    "    f.write(f's3_model_path={s3_model_path}\\n')\n",
    "\n",
    "print(\"\\n✅ vars.txt created with deployment variables:\")\n",
    "with open(\"vars.txt\", \"r\") as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b8c9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"✅ Model Save Complete!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nModel uploaded to: s3://{BUCKET_NAME}/{pipeline_key}\")\n",
    "print(f\"Model version: {model_version}\")\n",
    "print(f\"MinIO endpoint: {MINIO_ENDPOINT}\")\n",
    "print(f\"\\nNext step: Deploy to OpenShift AI (run notebook 3)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
